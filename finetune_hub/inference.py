import torch
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
from peft import PeftModel
from PIL import Image
import os

class InferenceEngine:
    """
    Handles the deployment phase of the VLM pipeline.
    
    This class is responsible for:
    1. Loading the base pre-trained model (PaliGemma).
    2. Loading and merging the fine-tuned LoRA adapter.
    3. Processing new input images and text prompts.
    4. Generating the final LaTeX output.
    """
    
    def __init__(self, base_model_id, adapter_path):
        """
        Initializes the inference engine by reconstructing the fine-tuned model.
        
        Args:
            base_model_id (str): Hugging Face ID of the base model (e.g., "google/paligemma-3b-pt-224").
            adapter_path (str): Local path to the saved LoRA adapter (from Training output_dir).
        """
        # Load the processor (Tokenizer + Image Resizer) associated with the base model.
        # This ensures our inputs match exactly what the model expects.
        self.processor = AutoProcessor.from_pretrained(base_model_id)
        
        # Load the Base Model in FP16 (Half Precision).
        # We load it onto the GPU automatically (device_map="auto") for fast inference.
        base_model = PaliGemmaForConditionalGeneration.from_pretrained(
            base_model_id,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        # Load the trained LoRA Adapter and attach it to the base model.
        # This acts like putting on "glasses" that specialize the model for Math LaTeX.
        self.model = PeftModel.from_pretrained(base_model, adapter_path)
        
        # Performance Optimization: Merge the adapter weights into the base model.
        # Instead of calculating Base + Adapter separately at runtime, we fuse them.
        # This makes inference faster (lower latency).
        self.model.merge_and_unload()
        
        # Set the model to Evaluation Mode.
        # This disables training-specific behaviors like Dropout to ensure deterministic results.
        self.model.eval()

    def generate(self, image_path: str, prompt_text: str, output_file: str = "output.tex"):
        """
        Runs the VLM on a single image to generate LaTeX code.
        
        Args:
            image_path (str): Path to the handwritten math image.
            prompt_text (str): The instruction prompt (e.g., "Convert this handwritten math to LaTeX.").
            output_file (str): Where to save the resulting text.
            
        Returns:
            str: The clean LaTeX string generated by the model.
        """
        # Load image and ensure it is in RGB format (removes alpha channels/transparency).
        image = Image.open(image_path).convert("RGB")
        full_prompt = f"{prompt_text}"
        
        # Preprocess inputs:
        # 1. Resizes/Normalizes the image.
        # 2. Tokenizes the text prompt.
        # 3. Moves tensors to the GPU (self.model.device).
        inputs = self.processor(text=full_prompt, images=image, return_tensors="pt").to(self.model.device)
        
        # Run Inference.
        # torch.no_grad() disables gradient calculation, saving massive amounts of memory since we aren't training.
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=100, # Limit the output length to prevent infinite loops.
                do_sample=False     # Greedy Decoding: Always pick the most likely next token (best for exact tasks like LaTeX).
            )
        
        # Decode the generated token IDs back into human-readable text.
        # skip_special_tokens=True removes internal markers like <eos> or <pad>.
        result = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        # Post-Processing:
        # The model often repeats the input prompt in its output. We strip it out to get just the answer.
        # Note: This simple string replace assumes the model outputs [Prompt] [Answer].
        cleaned_result = result.replace(full_prompt, "").strip()
        
        # Save the result to a file for easy use in LaTeX editors.
        with open(output_file, "w") as f:
            f.write(cleaned_result)
        
        return cleaned_result