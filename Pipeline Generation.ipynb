{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1b3WXtdOd5TlEIXqkuqf3QjR0l2d0p6Xw","authorship_tag":"ABX9TyPqLzCS5CECaMglLfVI4bAV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# !mkdir -p \"/content/drive/MyDrive/Colab Notebooks/Vee/finetune_hub\"\n","# print(\"Folder created successfully on Google Drive!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pn8eUOtc2peO","executionInfo":{"status":"ok","timestamp":1767603122647,"user_tz":-300,"elapsed":1821,"user":{"displayName":"Nabeel Shan","userId":"10782902425769176748"}},"outputId":"50ef55e4-55ba-4edd-d20a-203eed5cc2f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/Vee"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHj4S0Tb3gDO","executionInfo":{"status":"ok","timestamp":1767619268630,"user_tz":-300,"elapsed":227,"user":{"displayName":"Nabeel Shan","userId":"10782902425769176748"}},"outputId":"69051136-fde3-473d-b10c-0b61ebd8db49"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Vee\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ds7jDAFRypAK"},"outputs":[],"source":["# !pip install -q -U torch torchvision torchaudio transformers datasets peft bitsandbytes accelerate"]},{"cell_type":"code","source":["# !mkdir -p finetune_hub"],"metadata":{"id":"_JSKv_vB0umk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X4pS86iaBlOV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile finetune_hub/config.py\n","from dataclasses import dataclass, field\n","from typing import Optional, List\n","\n","@dataclass\n","class ModelConfig:\n","    # Model & Data Settings\n","    model_id: str = \"google/paligemma-3b-pt-224\"\n","    dataset_id: str = \"deepcopy/MathWriting-human\"\n","    prompt_text: str = \"Convert this handwritten math to LaTeX.\"\n","\n","    # Quantization & LoRA\n","    use_4bit: bool = True\n","    lora_rank: int = 16\n","    lora_alpha: int = 32\n","    lora_dropout: float = 0.05\n","    target_modules: List[str] = field(default_factory=lambda: [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"])\n","\n","    # Training Hyperparameters\n","    learning_rate: float = 2e-4\n","    batch_size: int = 4\n","    gradient_accumulation_steps: int = 4\n","    num_train_epochs: int = 1\n","    save_steps: int = 25\n","\n","    # System\n","    max_seq_length: int = 512\n","    output_dir: str = \"./math_vlm_adapter\"\n","    logging_steps: int = 5\n","    task_type: str = \"CAUSAL_LM\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXZfBTl-12EX","executionInfo":{"status":"ok","timestamp":1767617093338,"user_tz":-300,"elapsed":42,"user":{"displayName":"Nabeel Shan","userId":"10782902425769176748"}},"outputId":"8cb0b762-cb96-44b1-e662-005a63eec019"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting finetune_hub/config.py\n"]}]},{"cell_type":"code","source":["%%writefile finetune_hub/adapter.py\n","import torch\n","from peft import LoraConfig, TaskType\n","from transformers import BitsAndBytesConfig\n","from .config import ModelConfig\n","\n","class AdapterFactory:\n","    @staticmethod\n","    def get_qlora_config(cfg: ModelConfig) -> BitsAndBytesConfig:\n","        return BitsAndBytesConfig(\n","            load_in_4bit=cfg.use_4bit,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=True\n","        )\n","\n","    @staticmethod\n","    def get_lora_config(cfg: ModelConfig) -> LoraConfig:\n","        return LoraConfig(\n","            r=cfg.lora_rank,\n","            lora_alpha=cfg.lora_alpha,\n","            target_modules=cfg.target_modules,\n","            lora_dropout=cfg.lora_dropout,\n","            bias=\"none\",\n","            task_type=cfg.task_type\n","        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8CD8cBpt3OwV","executionInfo":{"status":"ok","timestamp":1767617106661,"user_tz":-300,"elapsed":302,"user":{"displayName":"Nabeel Shan","userId":"10782902425769176748"}},"outputId":"5819959b-75f1-447f-e4c8-0bad84a041af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting finetune_hub/adapter.py\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"6ArddAkp385g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile finetune_hub/engine.py\n","import torch\n","from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n","from peft import get_peft_model, prepare_model_for_kbit_training\n","from .config import ModelConfig\n","from .adapter import AdapterFactory\n","\n","class VLMEngine:\n","    def __init__(self, cfg: ModelConfig):\n","        self.cfg = cfg\n","        self.processor = None\n","        self.model = None\n","\n","    def load_model(self):\n","        print(f\"Loading Base Model: {self.cfg.model_id}...\")\n","        bnb_config = AdapterFactory.get_qlora_config(self.cfg)\n","\n","        self.model = PaliGemmaForConditionalGeneration.from_pretrained(\n","            self.cfg.model_id,\n","            quantization_config=bnb_config,\n","            device_map=\"auto\",\n","            dtype=torch.float16\n","        )\n","\n","        self.processor = AutoProcessor.from_pretrained(self.cfg.model_id)\n","\n","        # Enable Gradient Checkpointing (Saves VRAM)\n","        self.model.gradient_checkpointing_enable()\n","        self.model = prepare_model_for_kbit_training(self.model)\n","\n","    def apply_adapter(self):\n","        lora_config = AdapterFactory.get_lora_config(self.cfg)\n","        self.model = get_peft_model(self.model, lora_config)\n","\n","        print(\"\\n--- Trainable Parameters ---\")\n","        self.model.print_trainable_parameters()\n","        return self.model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byHdjHbQ4BZB","executionInfo":{"status":"ok","timestamp":1767619330577,"user_tz":-300,"elapsed":41,"user":{"displayName":"Nabeel Shan","userId":"10782902425769176748"}},"outputId":"457fedfe-f8b2-472a-f3b6-fff341c36053"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting finetune_hub/engine.py\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pOnfIios4B8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile finetune_hub/data.py\n","import torch\n","from datasets import load_dataset\n","from PIL import Image\n","\n","class DataProcessor:\n","    def __init__(self, processor, cfg):\n","        self.processor = processor\n","        self.cfg = cfg\n","\n","    def load_data(self, split=\"train\", limit=None):\n","        print(f\"Loading dataset: {self.cfg.dataset_id}...\")\n","        ds = load_dataset(self.cfg.dataset_id, split=split)\n","        if limit:\n","            ds = ds.select(range(limit))\n","        return ds\n","\n","    def collate_fn(self, examples):\n","        # Dynamic prompt from config\n","        texts = [f\"{self.cfg.prompt_text}\" for _ in examples]\n","        images = [ex[\"image\"].convert(\"RGB\") for ex in examples]\n","        labels = [ex[\"latex\"] for ex in examples]\n","\n","        inputs = self.processor(\n","            text=texts,\n","            images=images,\n","            suffix=labels,\n","            return_tensors=\"pt\",\n","            padding=\"longest\",\n","            truncation=True,\n","            max_length=self.cfg.max_seq_length\n","        )\n","\n","        return inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RoSIKEMx4EUc","executionInfo":{"status":"ok","timestamp":1767617129981,"user_tz":-300,"elapsed":552,"user":{"displayName":"Nabeel Shan","userId":"10782902425769176748"}},"outputId":"d2bc9514-bb1c-446f-e3ea-d722972cf335"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting finetune_hub/data.py\n"]}]},{"cell_type":"code","source":["%%writefile finetune_hub/trainer.py\n","import transformers\n","from transformers import Trainer, TrainingArguments\n","from .config import ModelConfig\n","\n","class TrainerWrapper:\n","    def __init__(self, model, processor, train_dataset, cfg: ModelConfig, data_collator):\n","        self.model = model\n","        self.processor = processor\n","        self.train_dataset = train_dataset\n","        self.cfg = cfg\n","        self.data_collator = data_collator\n","\n","    def train(self):\n","        args = TrainingArguments(\n","            output_dir=self.cfg.output_dir,\n","            per_device_train_batch_size=self.cfg.batch_size,\n","            gradient_accumulation_steps=self.cfg.gradient_accumulation_steps,\n","            warmup_steps=10,\n","            num_train_epochs=self.cfg.num_train_epochs,\n","            learning_rate=self.cfg.learning_rate,\n","            logging_steps=self.cfg.logging_steps,\n","            optim=\"paged_adamw_8bit\",\n","            save_strategy=\"steps\",\n","            save_steps=self.cfg.save_steps,\n","            fp16=True,\n","            report_to=\"none\",\n","            remove_unused_columns=False\n","        )\n","\n","        trainer = Trainer(\n","            model=self.model,\n","            args=args,\n","            train_dataset=self.train_dataset,\n","            data_collator=self.data_collator\n","        )\n","\n","        print(\"Starting Training...\")\n","        trainer.train()\n","\n","        print(f\"Saving Trainer State to {self.cfg.output_dir}...\")\n","        trainer.save_state()\n","\n","        print(f\"Saving Adapter to {self.cfg.output_dir}...\")\n","        self.model.save_pretrained(self.cfg.output_dir)\n","        self.processor.save_pretrained(self.cfg.output_dir)\n","\n","        return trainer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqv5xUfg4G1D","executionInfo":{"status":"ok","timestamp":1767619313148,"user_tz":-300,"elapsed":69,"user":{"displayName":"Nabeel Shan","userId":"10782902425769176748"}},"outputId":"03c37b3a-9723-4850-84e7-95c1f66cf628"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting finetune_hub/trainer.py\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"grBV6xVq4HYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile finetune_hub/inference.py\n","import torch\n","from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n","from peft import PeftModel\n","from PIL import Image\n","import os\n","\n","class InferenceEngine:\n","    def __init__(self, base_model_id, adapter_path):\n","        self.processor = AutoProcessor.from_pretrained(base_model_id)\n","\n","        base_model = PaliGemmaForConditionalGeneration.from_pretrained(\n","            base_model_id,\n","            device_map=\"auto\",\n","            torch_dtype=torch.float16\n","        )\n","        self.model = PeftModel.from_pretrained(base_model, adapter_path)\n","        self.model.merge_and_unload()\n","        self.model.eval()\n","\n","    def generate(self, image_path: str, prompt_text: str, output_file: str = \"output.tex\"):\n","        image = Image.open(image_path).convert(\"RGB\")\n","        full_prompt = f\"{prompt_text}\"\n","\n","        inputs = self.processor(text=full_prompt, images=image, return_tensors=\"pt\").to(self.model.device)\n","\n","        with torch.no_grad():\n","            generated_ids = self.model.generate(\n","                **inputs,\n","                max_new_tokens=100,\n","                do_sample=False\n","            )\n","\n","        result = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","        # Clean up the output to remove the prompt\n","        cleaned_result = result.replace(full_prompt, \"\").strip()\n","\n","        with open(output_file, \"w\") as f:\n","            f.write(cleaned_result)\n","\n","        return cleaned_result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UlXF9qiS4JXC","executionInfo":{"status":"ok","timestamp":1767617176513,"user_tz":-300,"elapsed":218,"user":{"displayName":"Nabeel Shan","userId":"10782902425769176748"}},"outputId":"14834b88-1034-4fcb-9291-ae181438ee6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting finetune_hub/inference.py\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EUeyyFOM4J2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile finetune_hub/__init__.py\n","from .config import ModelConfig\n","from .engine import VLMEngine\n","from .data import DataProcessor\n","from .trainer import TrainerWrapper\n","from .inference import InferenceEngine\n","\n","__all__ = [\n","    \"ModelConfig\",\n","    \"VLMEngine\",\n","    \"DataProcessor\",\n","    \"TrainerWrapper\",\n","    \"InferenceEngine\"\n","]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UgPyrpSS4Lt1","executionInfo":{"status":"ok","timestamp":1767617181543,"user_tz":-300,"elapsed":200,"user":{"displayName":"Nabeel Shan","userId":"10782902425769176748"}},"outputId":"5ca8e183-21a2-43ab-c9b4-fb39002d61a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting finetune_hub/__init__.py\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gMQc0jC74MWa"},"execution_count":null,"outputs":[]}]}